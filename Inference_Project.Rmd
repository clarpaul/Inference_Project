---
title: 'Class Project in Two Parts: Central Limit Theorem & Inferential Data Analysis'
author: "Paul Clark"
date: "February 5, 2017"
output:
  pdf_document:
    fig_caption: yes
    fig_height: 3.15
    number_sections: yes
    toc: yes
    toc_depth: 5
  html_notebook:
    number_sections: yes
    toc: yes
    toc_depth: 5
---
  
This document addresses a project assignment with two parts: Part 1, which is a simulation and investigation of the law of large numbers and the Central Limit Theorem, and Part 2, which is an exercise in Inferential analysis.  

# Central Limit Theorem (Simulation)

## Overview

We investigate the *exponential* distribution in R and compare it with the Central Limit Theorem. The distribution is simulated via `rexp(n, lambda)`, and both mean and standard deviation are `1/lambda`. We set `lambda = 0.2` and investigate the distribution of 1000 samples of 40 exponentials each.

We first load packages needed for the analysis.
```{r load_packages}
library(ggplot2)
library(tidyr)
```


## Simulations
We will show the properties of the distribution of the mean and variance of samples of 40 simulated random exponentials.  
```{r simulate}
# We simulate S = 1000 samples of N = 40 exponentials each, all with lambda = 0.2
S <- 1000
N <- 40
lambda <- 0.2
set.seed(1234)
sim_data <- matrix(rexp(n = S*N, rate = lambda), S, N)
means <- apply(sim_data, 1, mean)
variances <-  apply(sim_data, 1, function(x) {sd(x)^2}) # sample var, N-1 in denominator
sim <- data.frame(population = rexp(1000, rate = lambda), means, variances)
str(sim)
```

### Sample Mean vs. Theoretical Mean
Next, we show our simulated distribution of sample means and compare it to the theoretical mean of the population.  
```{r histogram_of_means, fig.cap = "Histogram of means of 1000 samples of 40 exponentials with theoretical mean 5",  message=FALSE}
# We plot a histogram of sample means
g <- ggplot(sim, aes(x = means)) + geom_histogram() 
g <- g + geom_vline(xintercept = mean(means), size = 1, color = "black") + 
     geom_vline(xintercept = median(means), size = 1, color = "red") +
     labs(title = paste0("The simulated mean lies just below the ",
     "theoretical mean of ", 1/lambda,"."," \n The mean (black line) and median ",
     "(red line) are approximately equal.")) + 
     theme(plot.title = element_text(face="italic", hjust=0.5))
print(g)
```
  
The vertical line shows the position of the empirical mean. It almost exactly equals the theoretical value $1/\lambda$, or `r 1/lambda`, for our lambda of `r lambda`.  To be precise, the difference $\mu_{simulated} - \mu_{theory}$ is `r format(round(diffmean <- mean(means) - 1/lambda, 2), nsmall=2)`, or `r format(round(100*diffmean*lambda, 2), nsmall=2)`% from theoretical.

### Sample Variance vs. Theoretical Variance
We show the sample variance and compare it to the theoretical variance of the population.  
```{r histogram_of_variances, fig.cap="Histogram of estimated population variances of 1000 samples of 40 exponentials", message=FALSE}
# Following slide 7 of lecture 5 of the statistical inference course, we plot a 
# histogram of sample variances and show its mean
g <- ggplot(sim, aes(x = variances)) + geom_histogram() 
g <- g + geom_vline(xintercept = mean(variances), size = 1) + 
     labs(title=paste0("The mean of simulated variances is just below the ",
                       "theoretical value of ", 1/lambda^2)) +
     theme(plot.title = element_text(face="italic", hjust=0.5)) +
     scale_x_continuous(breaks = seq(from = 0, to = 90, by = 10))
print(g)
```
  
The vertical line shows the position of the empirical mean of the distribution of estimated population variances. The mean is again close to the theoretical value of $1/\lambda^2$, or `r 1/lambda^2`.  To be precise, the difference $\sigma^2_{simulated}-\sigma^2_{theory}$ is `r (round(diffvar <- mean(sim$variances) - 1/lambda^2,2))`, or `r round(100*diffvar*lambda^2,2)`% from theoretical.  

We can also estimate population variance using the variance of the simulated means. By the properties of variances, $\sigma^2_{population} = N\sigma^2_{mean(N)}$. This estimate gives `r (format(round(popvarsim_means<-N*var(sim$means),2), nsmall = 2))`, which differs by `r (format(round(diffpopvarsim_means <- popvarsim_means - 1/lambda^2,2),nsmall = 2))` from the theoretical, or `r format(round(100*diffpopvarsim_means*lambda^2,2), nsmall = 2)`%.

## Normality
We now show that the distribution of means is trending away from exponential toward normal.  Our assessment will be primarily graphical.
```{r means_and_pop_histograms, fig.cap = "Histograms of 1000 Population Values and 1000 Means of random exponentials", message=FALSE}
# First, we reformat the data for use in ggplot
long_sim <- gather(sim, key = Distribution, value = Measure, population:variances)
# We construct a two-facet plot contrasting the randomly sampled distribution
# of means with a randomly sampled distribution of the population
long_sim_sub <- with(long_sim, long_sim[!Distribution %in% "variances",])
g <- ggplot(data = long_sim_sub, aes(x = Measure)) + geom_histogram() +
     facet_grid(Distribution~.) + labs(title = 
     "The means trend toward normal, while the population values are far from it") +
     theme(plot.title = element_text(face="italic", hjust=0.5))
print(g)
```
  
By visual inspection, the population distribution is severely right-skewed and not normal. Therefore, we focus our evaluation of normality on the distribution of means.  

### Equality of the Mean and Median
The Mean and Median of a normal distribution should be equal. This is approximately true for our distribution of means. In **Figure 1**, the black line represents the mean of means, whereas the red line represents the median. 

### Quantiles of sample distribution vs. quantiles of standard normal

Here we compare the quantiles of the distribution of the mean with those of standard normal.  

#### Explanation of Q-Q plot
In the plot below, our sample data is plotted on the horizontal axis against corresponding normal quantiles (standard deviations from standard normal) associated with the Empirical Cumulative Densities ($ECD_{q}\:'s$) of the sample data at each quantile. To obtain the cumulative densities, sample data points $i$ are ascendingly ordered from $1 \:to\: n$, then each point $i$ is related to a cumulative probability associated with the mid-point between points $i$ and $i-1$, calculated as $ECD_{q=5}=(i-0.5)/n$, where $n$ is the number of sampled values. This way, for example, point $i = 500$, which has sample quantile (data-value) $\approx 5$, is associated with cumulative probability `ECD = pnorm(q=5, mean=5, sd = sqrt(var(x)))` $\approx$ `0.4995 = (500-0.5)/1000`, and so on. That relationship is inverted (via `qnorm(p)`) to make connections with the *Standard Normal Quantiles* of the chart.

#### Q-Q plot for assessment of normality of the distribution of means
```{r quantiles_of_distribution, fig.cap="Plot of data against corresponding quantiles of standard normal"}
qqnorm(sim$means, main = paste0("Our data is verging on normal, with some",
        "\nleft compression and right skew"), ylab = "Data Values in the Sample", 
       xlab = "Normal Quantiles for Empirical Cumulative Density of Data (qnorm(ECD))")
qqline(sim$means)
```
  
The plot of sample data vs. corresponding quantiles of standard normal would be completely linear if our sample data was completely normal.  However, we see from the Q-Q plot above that as you move down from center (sample quantile ~$1/\lambda$=`r 1/lambda`), values of the data sample below 4 are traversed more slowly than linear with corresponding standard normal quantiles, indicating left compression (more probability density spread across a smaller range of values). And as we move up from center, the data traverses values faster than linear with the normal quantiles, indicating probability mass more right skewed than normal.  In summary, the distribution is not exactly normal, but retains relics of the underlying population distribution, which strongly exhibits this compression and skew (see bottom plot in *Figure 3*).

#### Number of standard deviations spanned by two normal quantiles of probability

However, the linear part of the Q-Q plot, between -1 and +1 normal quantiles attests to the fact that the data is still trending toward normal.  For example, we can compute the number of standard deviations of the mean between -1 and +1 standard normal quantiles from the center of the distribution, which should be 2 for a normal distribution.
```{r}
data_quantiles_1_stdnorm <- quantile(sim$means, probs = c(pnorm(q=-1),pnorm(q=1))) 
(med_plus_minus <- (data_quantiles_1_stdnorm - median(sim$means))/sqrt(var(sim$means)))
```
The number of standard deviations inside plus or minus 1 normal quantile of center (`r round(100*(pnorm(q=1)-pnorm(q=-1)),0)`%) of our distribution is just `r format(round(100*(med_plus_minus[2]-med_plus_minus[1] - 2)/2,2),nsmall=2)`% from 2. Above, the names attribute of the vector showing the values of the normal quantiles are the cumulative probabilities associated with the quantiles 1 left and 1 right of the median ($\pm\sqrt{variance(x)}$).  

# Basic Inferential Data Analysis  
## Overview
We now analyze the `ToothGrowth` data in the R `datasets` package via basic exploratory  analyses, summaries, and statistical tests to compare tooth growth by `supp` and `dose`. 
We first load the primary package needed in this analysis:
```{r}
library(datasets)
```
## Exploratory Data Analysis
We now perform basic summaries.
```{r}
data("ToothGrowth")
str(ToothGrowth)
with(ToothGrowth, table(supp, dose))
```
The results of `table()` most clarify the structure of the data. There are 10 subjects for each unique combination of `supp` and `dose`.  Guinea pigs were given one of 3 dosages of vitamin C daily (`dose`), in one of two forms (`supp`), and their tooth growth was measured.

Since we will just be performing T-tests between the various groups, it makes sense to re-cast `dose` as a factor.
```{r dose_factor}
ToothGrowth$dose <- factor(as.character(ToothGrowth$dose), levels = c("0.5", "1", "2"))
```
A box-plot reveals the data's major trends (**Figure 5**).
```{r boxplot, fig.cap="Boxplot of tooth length vs. vitamin delivery at different dosing"}
g <- ggplot(ToothGrowth, aes(x = supp, y = len, color = supp)) +
     geom_boxplot() + facet_grid(. ~ dose) + 
     labs(title="A boxpot clearly depicts underlying trends from both variables") +
     theme(plot.title = element_text(face="italic", hjust=0.5))
print(g)
```
  
Teeth appear to grow longer on higher doses, and, except at `dose = 2`, they grow longer on OJ (orange juice) than on VC (ascorbic acid).  

## Tests of Significance of Inter-Group Differences
We will test the statistical significance of the above observations using one-sided T-tests. To verify the graphically observed trends, we will take the following strategy:  

  1. Test two `dose` effects for each of the two fixed `supp` levels: 
     * Differences between dose levels 0.5 and 1
     * Differences between dose levels 1 and 2
  2. Within each of the three `dose` levels, test the difference between OJ and VC  
  
This means we will be carrying out 7 tests. We will extract P-values for each test, and reject the null at level $\alpha$ = 0.025.  

First, we carry out the inter-dose tests...
```{r dosage_effect}
# Function for t.tests
t.test_teeth <- function(dose2, dose1, supp2, supp1){
        t.test(with(ToothGrowth, len[dose == dose2 & supp == supp2]), 
               with(ToothGrowth, len[dose == dose1 & supp == supp1]),
                          alt = "greater")$p.value
}
# Construct a data frame associated with the dosage effect
(dosage.effect <- data.frame(dose.change = c("0.5 to 1", "1 to 2", "0.5 to 1", "1 to 2"),
                            supp.choice = c("OJ", "OJ", "VC", "VC"),
                            pvalue = round(c(
                                    t.test_teeth("1","0.5", "OJ", "OJ"),
                                    t.test_teeth("2","1","OJ","OJ"),
                                    t.test_teeth("1","0.5", "VC", "VC"),
                                    t.test_teeth("2","1","VC","VC")
                                    ),4)
                            )
)
```
  
The upward trend in tooth length with increasing dosage appears significant, whether delivering vitamin C via orange juice or ascorbic acid.

Now, we test supplement changes (choice of OJ or VC) at fixed dosage.
```{r}
# Construct and print a data frame associated with the VC to OJ effect
(VC_to_OJ.effect <- data.frame(dose.choice = c("0.5", "1", "2"),
                              pvalue = round(c(
                                    t.test_teeth("0.5","0.5", "OJ", "VC"),
                                    t.test_teeth("1","1","OJ","VC"),
                                    t.test_teeth("2","2","OJ","VC")
                                    ),4)
                              )
)
```
Substitution of orange juice (OJ) for ascorbic acid (VC) appears significantly associated with increase in tooth length for dosing of 0.5 and 1, but not for 2.

## Key Assumptions

  *  For applicability of the t distribution, the underlying data should theoretically be normal
  *  The guinea pigs are randomized across the various combinations of `supp` and `dose`
  *  The staff who measure tooth length are blind to the experimental conditions of each guinea pig



